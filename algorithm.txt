I used two jobs to solve this problem. The first mapreduce job writes the relative frequencies of all the words which co-occured. This is written into file named 'output1' into HDFS. After this, the second job starts. It reads the input from 'output1' and reverse sorts based on relative frequencies. Final result is written into file named 'output' into HDFS. As professor Chase Wu instructed in the class, word pairs with relative frequecy of 1.0 are not included in the final output. For each word, emit the word along with adjacent words (both left and right words, one by one) after lower case conversion of each word. Also, emit the word individually with a '*', with the word count, to keep track of the total word count. The combiner simply aggregates the word counts for word pairs and individual word, before sending to reducer. The Partitioner sends the adjacent words with the same key to the same reducer. The reducer1 counts the total word count with the help of '*' flag, and also the adjacent word counts. The reducer1 writes the relative frequency into HDFS (output1) as ratio of words co-occured and the total word count. The 2nd Mapper reads the above output and emits the swapped key values: Relative frequency is the new key, and Adjacent word pairs are values. DescendingKeyComparator reverse sorts the relative frequencies, before sending to the only reducer (reducer2). The 2nd reducer, after ignoring the word pairs with relative frequency of 1.0, writes the top 100 word pairs with highest relative frequencies into HDFS (output). And finally we copied results from HDFS(output) to top100.txt.
